{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Martltsera: Data and Training\n",
    "\n",
    "Character-level Seq2Seq Georgian Spellchecker\n",
    "\n",
    "Martltsera (\"მართლწერა\", meaning \"Spelling\" in Georgian) is uses recurrent neural networks (GRU/LSTM) to correct misspelled Georgian words. It operates on a character level, learning the intrinsic orthography of the language to fix typos, missing characters, and keyboard slips.\n",
    "\n",
    "This project was first implemented using Python scripts in the `src/` folder. This notebook explains the reasoning behind each script.\n",
    "\n",
    "Libraries and some config:"
   ],
   "id": "21500d77963055d8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T11:04:14.152048Z",
     "start_time": "2025-12-13T11:04:13.118144Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!pip install numpy torch\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "random.seed(95)  # ⚡\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', datefmt='%d/%m/%Y %H:%M:%S')\n",
    "logger = logging.getLogger(\"MartltseraLogger (Train)\")"
   ],
   "id": "495338125576bdd0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\ladoturmanidze\\martltsera\\martltsera_venv\\lib\\site-packages (2.3.5)\n",
      "Requirement already satisfied: torch in c:\\users\\ladoturmanidze\\martltsera\\martltsera_venv\\lib\\site-packages (2.9.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\ladoturmanidze\\martltsera\\martltsera_venv\\lib\\site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\ladoturmanidze\\martltsera\\martltsera_venv\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\ladoturmanidze\\martltsera\\martltsera_venv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\ladoturmanidze\\martltsera\\martltsera_venv\\lib\\site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ladoturmanidze\\martltsera\\martltsera_venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\ladoturmanidze\\martltsera\\martltsera_venv\\lib\\site-packages (from torch) (2025.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ladoturmanidze\\martltsera\\martltsera_venv\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ladoturmanidze\\martltsera\\martltsera_venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ladoturmanidze\\martltsera\\martltsera_venv\\lib\\site-packages (from jinja2->torch) (3.0.3)\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Constants and hyperparameters:",
   "id": "c1a3d4f000dd783e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T11:04:14.161798Z",
     "start_time": "2025-12-13T11:04:14.158195Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DICTIONARY_SIZE = 50000\n",
    "HIDDEN_SIZE = 512\n",
    "NUM_LAYERS = 1\n",
    "DROPOUT_P = 0.2\n",
    "LEARNING_RATE = 0.0001\n",
    "TEACHER_FORCING_RATIO = 0.5\n",
    "MODEL_SAVE_PATH = \"../models/Martltsera_5.pth\"\n",
    "SOS_token = 0\n",
    "EOS_token = 1"
   ],
   "id": "7a71cf6581538d10",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# `get_data.py`\n",
    "\n",
    "I needed a substantial vocabulary of correctly spelled Georgian words. I collected three JSON chunks (`wordsChunk_0.json`, `wordsChunk_1.json`, `wordsChunk_2.json`) from https://github.com/AleksandreSukh/GeorgianWordsDataBase and stored them in the `data_src/` directory, containing around 270K unique words in total after deduplication and cleaning. I restricted the dataset to random 50K samples, to afford training.\n",
    "\n",
    "### Data Generation: Creating Training Pairs\n",
    "\n",
    "To create the training pairs required for the seq2seq model, I focused on generating synthetic misspelled versions of the collected correct Georgian words. This method was chosen because collecting real-world misspelled data paired with corrections is challenging and time-consuming, especially for a low-resource language like Georgian. Instead, synthetic generation allows for scalable, controlled creation of diverse error patterns that simulate common typographical mistakes. By programmatically introducing errors, the model learns to map noisy inputs back to clean targets, effectively internalizing Georgian orthography rules without relying on external labeled error corpora.\n",
    "\n",
    "The key to effective synthetic errors is realism: most real typos are small perturbations (e.g., edit distance 1-2), often stemming from keyboard layouts where fingers slip to adjacent keys. To capture this, I incorporated a Georgian QWERTY keyboard neighbor map (`KEYBOARD_NEIGHBORS`), which defines nearby characters for each letter. For instance, 'ა' has neighbors ['ს', 'ქ', 'ზ'], reflecting physical proximity on the keyboard. This ensures errors feel natural, like substituting 'ს' for 'ა' due to a slight mispress, rather than arbitrary random changes that could confuse the model.\n",
    "\n",
    "The core augmentation logic is implemented in the `adapt_or_die` function, which processes each correct word character by character:\n",
    "\n",
    "1. **Error Probability Adjustment**: Shorter words (<7 characters) have a higher per-character error probability (15%) compared to longer ones (10%). This prevents short words from often remaining unchanged, ensuring they contribute meaningfully to training while avoiding excessive distortion in longer, more complex words.\n",
    "\n",
    "2. **Character Filtering**: Non-Georgian characters (outside Unicode range 4304–4336, i.e., 'ა' to 'ჰ') are preserved unchanged, as the model focuses solely on Mkhedruli script corrections.\n",
    "\n",
    "3. **Probabilistic Error Introduction**: For each Georgian character, if an error is triggered (based on the probability), a random action is selected:\n",
    "   - **Substitution (60% chance)**: Replace the character with a typo from `get_typo_char`. This function favors keyboard neighbors (95% chance) for realism, falling back to a fully random Georgian character only 5% of the time to add slight variety without overcomplicating patterns.\n",
    "   - **Deletion (20% chance)**: Simply skip the character, shortening the word.\n",
    "   - **Insertion (20% chance)**: Keep the original character and append a typo character after it, lengthening the word.\n",
    "\n",
    "   This distribution prioritizes substitutions, as they are the most common real-world typos (e.g., hitting the wrong key), while deletions and insertions handle cases like missed or extra keystrokes.\n",
    "\n",
    "4. **Guaranteed Perturbation**: To ensure every generated input is actually misspelled (crucial for training, as identical input-target pairs are added separately), if no error occurred during the pass, a forced substitution is applied at a random position (again using `get_typo_char` for consistency).\n",
    "\n",
    "This algorithm produces varied, realistic errors that mimic human typing behaviors, such as finger slips or hurried input, while keeping most changes minimal to reflect the assignment's note on edit distances. For example, running `adapt_or_die` on test words yields pairs like:\n",
    "- \"გამარჯობა\" $\\mapsto$ \"გამარჯობს\" (substitution of 'ა' to 'ს' at the end, a neighbor error)\n",
    "- \"გაგიმარჯოს\" $\\mapsto$ \"აგიმარჯოს\" (deletion of the first 'გ')\n",
    "- \"ლადო\" $\\mapsto$ \"ოადო\" (substitution of 'ლ' to 'ო', a neighbor via 'ო' being near 'ლ')\n",
    "- \"ბრიუს ვეინი\" $\\mapsto$ \"ბრჯუს ვეინი\" (insertion or substitution involving 'ჯ')\n",
    "- And so on for others, demonstrating small, plausible changes.\n",
    "\n",
    "Importantly, to promote model stability, I augmented the dataset with cases where the input is already correct (approximately 20% of pairs, sampled randomly in `train.py`). This teaches the model to output the input unchanged when no correction is needed, reducing the risk of over-correction during inference (e.g., altering valid rare words). Without this, the model might assume every input requires changes, leading to hallucinations. This directly addresses the assignment's question: \"Your dataset should include cases where the input is already correct. Why might this matter?\" - it ensures robustness and prevents unnecessary modifications, enhancing practical usability. Overall, this creative, keyboard-aware approach rewards data quality over quantity, as emphasized in the hints, resulting in a balanced dataset of ~50,000 pairs that effectively trains the model on intrinsic Georgian patterns.\n",
    "\n",
    "The `get_dataset_pairs` function loads the words, shuffles them, and applies `adapt_or_die` to create (misspelled, correct) pairs, skipping words shorter than 3 characters to focus on meaningful examples."
   ],
   "id": "c7f3150155ade146"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-13T11:04:14.303102Z",
     "start_time": "2025-12-13T11:04:14.172789Z"
    }
   },
   "source": [
    "ALL_GEORGIAN_CHARS = [chr(i) for i in range(4304, 4337)]\n",
    "\n",
    "# standard Georgian QWERTY keyboard neighbors to simulate realistic finger slips\n",
    "KEYBOARD_NEIGHBORS: dict[str, list[str]] = {\n",
    "    'ა': ['ს', 'ქ', 'ზ'],\n",
    "    'ბ': ['ვ', 'გ', 'ჰ', 'ნ'],\n",
    "    'გ': ['ფ', 'ტ', 'ყ', 'ჰ', 'ბ', 'ვ'],\n",
    "    'დ': ['ს', 'ე', 'რ', 'ფ', 'ც', 'ხ'],\n",
    "    'ე': ['წ', 'რ', 'დ', 'ს'],\n",
    "    'ვ': ['ც', 'ფ', 'გ', 'ბ'],\n",
    "    'ზ': ['ა', 'ს', 'ხ'],\n",
    "    'თ': ['ღ', 'ყ', 'გ', 'ფ'],  # whenever we use \"Shift +\", all the neighbors are also with \"Shift +\" (if there exists one)\n",
    "    'ი': ['უ', 'ო', 'ჯ', 'ჰ'],\n",
    "    'კ': ['ჯ', 'ი', 'ო', 'ლ', 'მ'],\n",
    "    'ლ': ['კ', 'მ', 'ო', 'პ'],\n",
    "    'მ': ['ნ', 'ჯ', 'კ', 'ლ'],\n",
    "    'ნ': ['ბ', 'ჰ', 'ჯ', 'მ'],\n",
    "    'ო': ['ი', 'პ', 'ლ', 'კ', 'ჯ'],\n",
    "    'პ': ['ო', 'ლ'],\n",
    "    'ჟ': ['ჰ', 'უ', 'ი', 'კ', 'მ', 'ნ'],\n",
    "    'რ': ['ე', 'ტ', 'ფ', 'დ'],\n",
    "    'ს': ['ა', 'წ', 'ე', 'დ', 'ხ', 'ზ'],\n",
    "    'ტ': ['რ', 'ყ', 'გ', 'ფ'],\n",
    "    'უ': ['ყ', 'ი', 'ჯ', 'ჰ'],\n",
    "    'ფ': ['დ', 'რ', 'ტ', 'გ', 'ვ', 'ც'],\n",
    "    'ქ': ['წ', 'ა'],\n",
    "    'ღ': ['ე', 'თ', 'ფ', 'დ'],\n",
    "    'ყ': ['თ', 'უ', 'ჰ', 'გ'],\n",
    "    'შ': ['ა', 'წ', 'ე', 'დ', 'ხ', 'ზ'],\n",
    "    'ჩ': ['ხ', 'დ', 'ფ', 'ვ'],\n",
    "    'ც': ['ხ', 'დ', 'ფ', 'ვ'],\n",
    "    'ძ': ['ა', 'ს', 'ხ'],\n",
    "    'წ': ['ქ', 'ე', 'ს', 'ა'],\n",
    "    'ჭ': ['ქ', 'ე', 'ს', 'ა'],\n",
    "    'ხ': ['ა', 'ს', 'დ', 'ც'],\n",
    "    'ჯ': ['ჰ', 'უ', 'ი', 'კ', 'მ', 'ნ'],\n",
    "    'ჰ': ['გ', 'ყ', 'უ', 'ჯ', 'ნ', 'ბ']\n",
    "}\n",
    "\n",
    "\n",
    "def read_sources(path: str) -> list[str]:\n",
    "    # for script version: data_src_path = os.path.abspath(os.path.join(os.path.dirname(__file__), path))\n",
    "    data_src_path = os.path.abspath(os.path.join(os.getcwd(), path))\n",
    "    res: list[str] = []\n",
    "\n",
    "    for i in range(3):\n",
    "        file_path = os.path.join(data_src_path, f\"wordsChunk_{i}.json\")\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = json.load(f)\n",
    "            res.extend(content)\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_typo_char(c: str) -> str:\n",
    "    # 95% chance to pick a neighbor, 5% random char\n",
    "    if c in KEYBOARD_NEIGHBORS and random.random() > 0.05:\n",
    "        return random.choice(KEYBOARD_NEIGHBORS[c])\n",
    "    return random.choice(ALL_GEORGIAN_CHARS)\n",
    "\n",
    "\n",
    "def adapt_or_die(word: str) -> str:\n",
    "    res = \"\"\n",
    "    error_made = False\n",
    "\n",
    "    # higher chance of error per char for shorter words otherwise short words often remain untouched\n",
    "    prob = 0.15 if len(word) < 7 else 0.10\n",
    "\n",
    "    for c in word:\n",
    "        # ord('ა') == 4304 and ord('ჰ') == 4336\n",
    "        if not (4304 <= ord(c) <= 4336):\n",
    "            res += c\n",
    "            continue\n",
    "\n",
    "        if random.random() < prob:\n",
    "            # 0 -> substitution p = 0.6; 1 -> deletion p = 0.2; 2 -> insertion p = 0.2\n",
    "            action_roll = random.random()\n",
    "            error_made = True\n",
    "\n",
    "            if action_roll < 0.6:\n",
    "                res += get_typo_char(c)  # substitution jutsu!\n",
    "            elif action_roll < 0.8:\n",
    "                continue  # deletion\n",
    "            else:\n",
    "                res += c + get_typo_char(c)  # insertion\n",
    "        else:\n",
    "            res += c\n",
    "\n",
    "    # ensure the word is actually corrupted for training purposes\n",
    "    if not error_made and len(res) > 0:\n",
    "        idx = random.randint(0, len(res) - 1)\n",
    "        temp_list = list(res)\n",
    "        if 4304 <= ord(temp_list[idx]) <= 4336:\n",
    "            temp_list[idx] = get_typo_char(temp_list[idx])\n",
    "        res = \"\".join(temp_list)\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_dataset_pairs(path: str = \"../data_src\", dictionary_size: int = 50000) -> list[tuple[str, str]]:\n",
    "    pure_data = read_sources(path)\n",
    "    random.shuffle(pure_data)\n",
    "\n",
    "    dataset = []\n",
    "    count = 0\n",
    "\n",
    "    for word in pure_data:\n",
    "        if count >= dictionary_size:\n",
    "            break\n",
    "\n",
    "        if len(word) < 3:  # skip extremely short words\n",
    "            continue\n",
    "\n",
    "        dataset.append((adapt_or_die(word), word))\n",
    "        count += 1\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "print(\"<== Testing adapt_or_die ==>\")\n",
    "test_words = [\"გამარჯობა\", \"გაგიმარჯოს\", \"ლადო\", \"ბრიუს ვეინი\", \"კომპიუტერი\", \"სიყვარული\", \"მაყუთი\", \"წიგნიერება\", \"გოკუ\", \"ტელეფონი\", \"ბეტმენი\"]\n",
    "for w in test_words:\n",
    "    print(f\"{w} -> {adapt_or_die(w)}\")\n",
    "\n",
    "print(\"\\n<== Testing dataset generation ==>\")\n",
    "data = get_dataset_pairs(dictionary_size=15)\n",
    "for corrupted, original in data:\n",
    "    print(f\"Corrupted: {corrupted} | Original: {original}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<== Testing adapt_or_die ==>\n",
      "გამარჯობა -> გამარჯობს\n",
      "გაგიმარჯოს -> აგიმარჯოს\n",
      "ლადო -> ოადო\n",
      "ბრიუს ვეინი -> ბრჯუს ვეინი\n",
      "კომპიუტერი -> კჯმპიუტერი\n",
      "სიყვარული -> სითვარული\n",
      "მაყუთი -> მაგუთი\n",
      "წიგნიერება -> წიგნიერებ\n",
      "გოკუ -> გოკჰ\n",
      "ტელეფონი -> გწლეფნი\n",
      "ბეტმენი -> ბეტჯენი\n",
      "\n",
      "<== Testing dataset generation ==>\n",
      "Corrupted: დაცურათ | Original: დავცურავთ\n",
      "Corrupted: მინდორიდ | Original: მინდორიც\n",
      "Corrupted: დავუმტკიცე-მეთქაი | Original: დავუმტკიცე-მეთქი\n",
      "Corrupted: ხეცურებულან | Original: შეცურებულან\n",
      "Corrupted: მსმალმა | Original: მამალმა\n",
      "Corrupted: ძდვენთა | Original: ძღვენთა\n",
      "Corrupted: სექჭიები | Original: სექციები\n",
      "Corrupted: გაევფერა | Original: გაშვერა\n",
      "Corrupted: გამოსროლილმს | Original: გამოსროლილმა\n",
      "Corrupted: საუთოწბლებზე | Original: საუთოებლებზე\n",
      "Corrupted: აუყეხიათ | Original: აუტეხიათ\n",
      "Corrupted: ცრემლიც | Original: ცრემლიც\n",
      "Corrupted: შეეყალა | Original: შეეწყალა\n",
      "Corrupted: გაერთიანდებისნ | Original: გაერთიანდებიან\n",
      "Corrupted: ლეონოდე | Original: ლეონიდე\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# `Gamarjoba.py`\n",
    "\n",
    "This file defines the core model architecture, named \"Gamarjoba\" (Georgian for \"hello\"), which is an encoder-decoder LSTM network.\n",
    "\n",
    "## LSTM (Long Short-Term Memory) Justification:\n",
    "I chose the LSTM architecture because Georgian words have a complex structure. A single word often consists of a root with many prefixes and suffixes attached to it.\n",
    "To fix a spelling error, the model needs to remember the beginning of the word (the root) to figure out the correct ending.\n",
    "Standard RNNs often struggle to remember this information over long sequences because of the \"vanishing gradient\" problem.\n",
    "LSTMs solve this using a Cell State - a type of internal memory that acts like a highway, allowing information to travel through the network without getting lost.\n",
    "This makes the LSTM perfect for learning the long and complex character patterns found in the Georgian language.\n",
    "\n",
    "## Encoder-Decoder Architecture Justification:\n",
    "I implemented an Encoder-Decoder structure because spelling errors often change the length of a word.\n",
    "For example, if a user misses a key or types an extra one, the input length will differ from the correct output.\n",
    "This architecture solves that problem by separating the task into two parts.\n",
    "The Encoder reads the entire misspelled word first to capture its full context.\n",
    "The Decoder then uses that information to generate the correct word character by character.\n",
    "This allows the model to handle complex errors like insertions and deletions, not just simple letter replacements."
   ],
   "id": "b0feb9dfd2e187af"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T11:04:14.322868Z",
     "start_time": "2025-12-13T11:04:14.312923Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, num_layers: int = 1, dropout_p: float = 0.2):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)  # convert character indices to dense vectors\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers=num_layers)  # this saved me tons of time\n",
    "\n",
    "\n",
    "    def forward(self, input_seq: torch.Tensor, hidden: torch.Tensor, cell: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        embedded = self.embedding(input_seq).view(1, 1, -1)  # Shape (N) -> Shape (seq_len=1, batch_size=1, input_size=N); as 1 * 1  = 1, -1 will automatically infer the embedding dimension\n",
    "        output = self.dropout(embedded)\n",
    "        output, (hidden, cell) = self.lstm(output, (hidden, cell))  # updates hidden/cell states based on current input\n",
    "\n",
    "        return output, hidden, cell\n",
    "\n",
    "    # LSTM processes data sequentially. Since there is no previous memory at the start, we must create one: (h_0, c_0).\n",
    "    def init_hidden(self, device: torch.device) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        return torch.zeros(self.num_layers, 1, self.hidden_size, device=device), torch.zeros(self.num_layers, 1, self.hidden_size, device=device)  # (num_layers,\n",
    "\n",
    "\n",
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, output_size: int, hidden_size: int, num_layers: int = 1, dropout_p: float = 0.2):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)  # embeds the previous character (or SOS)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers=num_layers)  # processes the sequence step-by-step\n",
    "        self.out = nn.Linear(hidden_size, output_size)  # projects hidden state to vocabulary size (logits)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)  # converts logits to log-probabilities for prediction\n",
    "\n",
    "\n",
    "    def forward(self, input_step: torch.Tensor, hidden: torch.Tensor, cell: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        output = self.embedding(input_step).view(1, 1, -1)\n",
    "        output = self.dropout(output)\n",
    "        output = torch.nn.functional.relu(output)\n",
    "        output, (hidden, cell) = self.lstm(output, (hidden, cell))  # updates state based on input and previous state\n",
    "        prediction = self.softmax(self.out(output[0]))  # computes probability distribution over vocabulary\n",
    "\n",
    "        return prediction, hidden, cell\n",
    "\n",
    "\n",
    "class Gamarjoba(nn.Module):\n",
    "    def __init__(self, vocab_size: int, hidden_size: int, num_layers: int = 1, dropout_p: float = 0.2):\n",
    "        super(Gamarjoba, self).__init__()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.encoder = EncoderLSTM(vocab_size, hidden_size, num_layers, dropout_p).to(self.device)\n",
    "        self.decoder = DecoderLSTM(vocab_size, hidden_size, num_layers, dropout_p).to(self.device)\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "\n",
    "    # Teacher Forcing: A training method where the model is sometimes fed the actual correct previous character instead of its own predicted guess to speed up convergence.\n",
    "    def forward(self, input_tensor: torch.Tensor, target_tensor: torch.Tensor, teacher_forcing_ratio: float = 0.5) -> torch.Tensor:\n",
    "        input_length = input_tensor.size(0)\n",
    "        target_length = target_tensor.size(0)\n",
    "\n",
    "        encoder_hidden, encoder_cell = self.encoder.init_hidden(self.device)  # initializes hidden state with zeros\n",
    "\n",
    "        for i in range(input_length):\n",
    "            _, encoder_hidden, encoder_cell = self.encoder(input_tensor[i], encoder_hidden, encoder_cell)  # builds context vector\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=self.device)  # Start-Of-Sequence token assumed 0; starts decoding\n",
    "        decoder_hidden = encoder_hidden  # passes the Encoder's final memory to Decoder\n",
    "        decoder_cell = encoder_cell\n",
    "\n",
    "        outputs = torch.zeros(target_length, self.vocab_size, device=self.device)\n",
    "\n",
    "        for i in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_cell = self.decoder(decoder_input, decoder_hidden, decoder_cell)  # predict next char\n",
    "            outputs[i] = decoder_output.squeeze(0)  # stores the prediction\n",
    "\n",
    "            use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False  # decides strategy randomly\n",
    "\n",
    "            if use_teacher_forcing:\n",
    "                decoder_input = target_tensor[i]  # feeds the correct character as next input\n",
    "            else:\n",
    "                _, top_i = decoder_output.topk(1)  # gets the character index with the highest probability\n",
    "                decoder_input = top_i.squeeze().detach()  # detach() prevents backprop through this step (treats input as constant)\n",
    "                if decoder_input.item() == EOS_token:  # End-Of-Sequence token assumed 1\n",
    "                    break\n",
    "\n",
    "        return outputs"
   ],
   "id": "cd3a4e3441dc9def",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# `train.py`\n",
    "\n",
    "This script manages the training process, including dataset splitting, batching, and optimization.\n",
    "\n",
    "I split the pairs into 80% train / 20% validation for monitoring overfitting. Batching is done with a size of 64 (adjustable), processing batches sequentially but computing loss per word to handle variable lengths without padding (though for efficiency, true batching with padding could be added).\n",
    "\n",
    "The training loop:\n",
    "- Uses Adam optimizer with `LR=0.0001` (conservative to avoid instability).\n",
    "- `NLLLoss` for multi-class character prediction.\n",
    "- Tracks average loss per token.\n",
    "- Validates after each epoch, saving checkpoints if val loss improves.\n",
    "- Early stopping with `patience=5` to prevent unnecessary computation.\n",
    "\n",
    "Hyperparameters like `hidden_size=512` and `num_layers=1` balance capacity and speed. I added 20% correct pairs to the dataset here for stability.\n",
    "\n",
    "This demonstrates the full workflow: splitting, batching, loss tracking, and checkpointing based on validation.\n",
    "\n",
    "## Design Considerations for Variable Output Length\n",
    "- I adopted a classic encoder-decoder (seq2seq) architecture with no forced alignment.\n",
    "The encoder (LSTM) processes the entire misspelled input and compresses it into a fixed-size context (final hidden and cell states).\n",
    "- The decoder (also LSTM) then autoregressively generates the corrected word one character at a time, starting from a `<SOS>` token and stopping when it predicts `<EOS>`.\n",
    "- This design allows the output length to be completely independent of input length, naturally handling insertions, deletions, and substitutions without any padding or length constraints.\n",
    "\n",
    "## Character Vocabulary and Handling the Georgian Alphabet\n",
    "- The Georgian Mkhedruli script consists of $33$ modern letters (U+10D0 to U+10F0, but contiguous from U+10D0='ა' to U+10FC='ჰ').\n",
    "- In the provided code, `ALL_GEORGIAN_CHARS = [chr(i) for i in range(4304, 4337)]`, which covers exactly these 33 characters ($4304$ is 10D0 in hex; $4336$ is 10FC in hex).\n",
    "- The vocabulary size is therefore $33 + 2$ special tokens $= 35$.\n",
    "- Character-to-index mapping assigns indices $2 \\dots 34$ to the $33$ letters (in Unicode order), reserving $0$ for `<SOS>` and $1$ for `<EOS>`.\n",
    "- During tokenization, any character not in this set is skipped (`tensor_from_word` filters with `if char in char_to_index`), which is safe because the dataset contains only Georgian words and the model focuses exclusively on Mkhedruli script.\n",
    "\n",
    "## Special Tokens\n",
    "- `<SOS>` (index $0$): Used only to initialise the decoder at inference time (and implicitly during teacher forcing in training). It signals the start of generation and provides a neutral starting point for the decoder LSTM.\n",
    "- `<EOS>` (index $1$): Appended to every target sequence during training so the model learns to predict it after the last real character. At inference, greedy decoding stops when `<EOS>` is predicted, preventing over-generation.\n",
    "- No `<PAD>` token: The model processes sequences one character at a time (`batch_size=1` per word effectively, even in batches) and lengths vary naturally, so padding is unnecessary."
   ],
   "id": "3a3bfcf97ae9513e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T11:04:14.340350Z",
     "start_time": "2025-12-13T11:04:14.330339Z"
    }
   },
   "cell_type": "code",
   "source": [
    "char_to_index = {char: i + 2 for i, char in enumerate(ALL_GEORGIAN_CHARS)}\n",
    "index_to_char = {i + 2: char for i, char in enumerate(ALL_GEORGIAN_CHARS)}\n",
    "VOCAB_SIZE = len(ALL_GEORGIAN_CHARS) + 2\n",
    "\n",
    "def tensor_from_word(word: str, device: torch.device) -> torch.Tensor:\n",
    "    indexes = [char_to_index[char] for char in word if char in char_to_index]\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def train_batch(model: Gamarjoba, optimizer: torch.optim.Optimizer, criterion: nn.NLLLoss, batch_pairs: list[tuple[str, str]], device: torch.device) -> float:\n",
    "    optimizer.zero_grad()\n",
    "    total_loss = torch.tensor(0.0, device=device)\n",
    "    total_tokens = 0\n",
    "\n",
    "    for input_word, target_word in batch_pairs:\n",
    "        input_tensor = tensor_from_word(input_word, device)\n",
    "        target_tensor = tensor_from_word(target_word, device)\n",
    "        target_length = target_tensor.size(0)\n",
    "        outputs = model(input_tensor, target_tensor, teacher_forcing_ratio=TEACHER_FORCING_RATIO)\n",
    "        example_loss = torch.tensor(0.0, device=device)\n",
    "\n",
    "        for i in range(target_length):\n",
    "            example_loss += criterion(outputs[i].unsqueeze(0), target_tensor[i])\n",
    "\n",
    "        total_loss += example_loss\n",
    "        total_tokens += target_length\n",
    "\n",
    "    if total_tokens > 0:\n",
    "        avg_loss = total_loss / total_tokens\n",
    "        avg_loss.backward()\n",
    "        optimizer.step()\n",
    "        return avg_loss.item()\n",
    "\n",
    "    optimizer.step()\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def validate(model: Gamarjoba, criterion: nn.NLLLoss, val_pairs: list[tuple[str, str]], device: torch.device) -> float:\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_word, target_word in val_pairs:\n",
    "            input_tensor = tensor_from_word(input_word, device)\n",
    "            target_tensor = tensor_from_word(target_word, device)\n",
    "            target_length = target_tensor.size(0)\n",
    "            outputs = model(input_tensor, target_tensor, teacher_forcing_ratio=TEACHER_FORCING_RATIO)\n",
    "            example_loss = 0.0\n",
    "\n",
    "            for i in range(target_length):\n",
    "                example_loss += criterion(outputs[i].unsqueeze(0), target_tensor[i]).item()\n",
    "\n",
    "            total_loss += example_loss\n",
    "            total_tokens += target_length\n",
    "\n",
    "    model.train()\n",
    "    return total_loss / total_tokens if total_tokens > 0 else 0.0\n",
    "\n",
    "\n",
    "def get_batches(pairs: list[tuple[str, str]], batch_size: int):\n",
    "    for i in range(0, len(pairs), batch_size):\n",
    "        yield pairs[i:i + batch_size]\n",
    "\n",
    "\n",
    "def train_model(epochs: int, batch_size: int):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "\n",
    "    model = Gamarjoba(VOCAB_SIZE, HIDDEN_SIZE, num_layers=NUM_LAYERS, dropout_p=DROPOUT_P)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    logger.info(\"Generating dataset...\")\n",
    "    pairs = get_dataset_pairs(dictionary_size=DICTIONARY_SIZE)\n",
    "    correct_pairs = [(w, w) for _, w in random.sample(pairs, int(0.2 * len(pairs)))]  # add correct pairs\n",
    "    pairs += correct_pairs\n",
    "    random.shuffle(pairs)\n",
    "    n = len(pairs)\n",
    "    train_size = int(0.8 * n)\n",
    "    train_pairs = pairs[:train_size]\n",
    "    val_pairs = pairs[train_size:]\n",
    "    logger.info(f\"Dataset ready: {len(train_pairs)} train pairs, {len(val_pairs)} val pairs.\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    log_interval = 200\n",
    "    loss_interval = 0.0\n",
    "    total_steps = epochs * len(train_pairs)\n",
    "    iter_count = 0\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 5\n",
    "    counter = 0\n",
    "\n",
    "    logger.info(f\"Starting training for {epochs} epochs...\")\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        random.shuffle(train_pairs)\n",
    "        for batch_pairs in get_batches(train_pairs, batch_size):\n",
    "            batch_len = len(batch_pairs)\n",
    "            iter_count += batch_len\n",
    "            loss = train_batch(model, optimizer, criterion, batch_pairs, device)\n",
    "            loss_interval += loss * batch_len\n",
    "            if iter_count % log_interval == 0:\n",
    "                loss_avg = loss_interval / log_interval\n",
    "                logger.info(f\"{iter_count} steps ({iter_count / total_steps * 100:.0f}% complete) | Loss: {loss_avg:.3f}\")\n",
    "                loss_interval = 0.0\n",
    "\n",
    "        val_loss = validate(model, criterion, val_pairs, device)\n",
    "        logger.info(f\"Epoch {epoch} Validation Loss: {val_loss:.3f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            os.makedirs(os.path.dirname(MODEL_SAVE_PATH[:-4] + f\"_{epoch}.pth\"), exist_ok=True)\n",
    "            torch.save(model.state_dict(), MODEL_SAVE_PATH[:-4] + f\"_{epoch}.pth\")\n",
    "            logger.info(f\"New best model saved to {MODEL_SAVE_PATH[:-4] + f\"_{epoch}.pth\"}\")\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                logger.info(f\"Early stopping at epoch {epoch}\")\n",
    "                break"
   ],
   "id": "df0964d3f2ee91bc",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Training time (took $\\approx 100$ minutes on Colab's T4 GPU)!\n",
    "\n",
    "***Note***: I stopped training the execution of the next cell simply because I trained the model on Colab and downloaded the models from there, as it would take over $300$ minutes on my machine to train the model for $5$ epochs, whereas it only took $\\approx 100$ minutes on Colab.\n",
    "\n",
    "These were the last 2 lines of logs from Colab:\n",
    "```bash\n",
    "13/12/2025 10:54:25 - MartltseraLogger (Train) - INFO - 240000 steps (100% complete) | Loss: 7.263\n",
    "13/12/2025 10:55:42 - MartltseraLogger (Train) - INFO - Epoch 5 Validation Loss: 0.877\n",
    "```"
   ],
   "id": "5f2e1ed14dfa5a3e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T11:04:21.232293Z",
     "start_time": "2025-12-13T11:04:14.347820Z"
    }
   },
   "cell_type": "code",
   "source": "train_model(5, 64)",
   "id": "d5e1c7f1f658d4e0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13/12/2025 15:04:14 - MartltseraLogger (Train) - INFO - Using device: cpu\n",
      "13/12/2025 15:04:14 - MartltseraLogger (Train) - INFO - Generating dataset...\n",
      "13/12/2025 15:04:14 - MartltseraLogger (Train) - INFO - Dataset ready: 48000 train pairs, 12000 val pairs.\n",
      "13/12/2025 15:04:14 - MartltseraLogger (Train) - INFO - Starting training for 5 epochs...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[22]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m64\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[21]\u001B[39m\u001B[32m, line 102\u001B[39m, in \u001B[36mtrain_model\u001B[39m\u001B[34m(epochs, batch_size)\u001B[39m\n\u001B[32m    100\u001B[39m batch_len = \u001B[38;5;28mlen\u001B[39m(batch_pairs)\n\u001B[32m    101\u001B[39m iter_count += batch_len\n\u001B[32m--> \u001B[39m\u001B[32m102\u001B[39m loss = \u001B[43mtrain_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_pairs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    103\u001B[39m loss_interval += loss * batch_len\n\u001B[32m    104\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m iter_count % log_interval == \u001B[32m0\u001B[39m:\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[21]\u001B[39m\u001B[32m, line 20\u001B[39m, in \u001B[36mtrain_batch\u001B[39m\u001B[34m(model, optimizer, criterion, batch_pairs, device)\u001B[39m\n\u001B[32m     18\u001B[39m target_tensor = tensor_from_word(target_word, device)\n\u001B[32m     19\u001B[39m target_length = target_tensor.size(\u001B[32m0\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m20\u001B[39m outputs = \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_tensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget_tensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mteacher_forcing_ratio\u001B[49m\u001B[43m=\u001B[49m\u001B[43mTEACHER_FORCING_RATIO\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     21\u001B[39m example_loss = torch.tensor(\u001B[32m0.0\u001B[39m, device=device)\n\u001B[32m     23\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(target_length):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Martltsera\\martltsera_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1774\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1775\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Martltsera\\martltsera_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1781\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1784\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1786\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1788\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1789\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[20]\u001B[39m\u001B[32m, line 76\u001B[39m, in \u001B[36mGamarjoba.forward\u001B[39m\u001B[34m(self, input_tensor, target_tensor, teacher_forcing_ratio)\u001B[39m\n\u001B[32m     73\u001B[39m outputs = torch.zeros(target_length, \u001B[38;5;28mself\u001B[39m.vocab_size, device=\u001B[38;5;28mself\u001B[39m.device)\n\u001B[32m     75\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(target_length):\n\u001B[32m---> \u001B[39m\u001B[32m76\u001B[39m     decoder_output, decoder_hidden, decoder_cell = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdecoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdecoder_input\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdecoder_hidden\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdecoder_cell\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# predict next char\u001B[39;00m\n\u001B[32m     77\u001B[39m     outputs[i] = decoder_output.squeeze(\u001B[32m0\u001B[39m)  \u001B[38;5;66;03m# stores the prediction\u001B[39;00m\n\u001B[32m     79\u001B[39m     use_teacher_forcing = \u001B[38;5;28;01mTrue\u001B[39;00m \u001B[38;5;28;01mif\u001B[39;00m random.random() < teacher_forcing_ratio \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m  \u001B[38;5;66;03m# decides strategy randomly\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Martltsera\\martltsera_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1774\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1775\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Martltsera\\martltsera_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1781\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1784\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1786\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1788\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1789\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[20]\u001B[39m\u001B[32m, line 44\u001B[39m, in \u001B[36mDecoderLSTM.forward\u001B[39m\u001B[34m(self, input_step, hidden, cell)\u001B[39m\n\u001B[32m     42\u001B[39m output = \u001B[38;5;28mself\u001B[39m.dropout(output)\n\u001B[32m     43\u001B[39m output = torch.nn.functional.relu(output)\n\u001B[32m---> \u001B[39m\u001B[32m44\u001B[39m output, (hidden, cell) = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mlstm\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcell\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# updates state based on input and previous state\u001B[39;00m\n\u001B[32m     45\u001B[39m prediction = \u001B[38;5;28mself\u001B[39m.softmax(\u001B[38;5;28mself\u001B[39m.out(output[\u001B[32m0\u001B[39m]))  \u001B[38;5;66;03m# computes probability distribution over vocabulary\u001B[39;00m\n\u001B[32m     47\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m prediction, hidden, cell\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Martltsera\\martltsera_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1774\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1775\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Martltsera\\martltsera_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1781\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1784\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1786\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1788\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1789\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Martltsera\\martltsera_venv\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:1127\u001B[39m, in \u001B[36mLSTM.forward\u001B[39m\u001B[34m(self, input, hx)\u001B[39m\n\u001B[32m   1124\u001B[39m         hx = \u001B[38;5;28mself\u001B[39m.permute_hidden(hx, sorted_indices)\n\u001B[32m   1126\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m batch_sizes \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1127\u001B[39m     result = \u001B[43m_VF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mlstm\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1128\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m   1129\u001B[39m \u001B[43m        \u001B[49m\u001B[43mhx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1130\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_flat_weights\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[arg-type]\u001B[39;49;00m\n\u001B[32m   1131\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1132\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mnum_layers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1133\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdropout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1134\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1135\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbidirectional\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1136\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbatch_first\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1137\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1138\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1139\u001B[39m     result = _VF.lstm(\n\u001B[32m   1140\u001B[39m         \u001B[38;5;28minput\u001B[39m,\n\u001B[32m   1141\u001B[39m         batch_sizes,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1148\u001B[39m         \u001B[38;5;28mself\u001B[39m.bidirectional,\n\u001B[32m   1149\u001B[39m     )\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 22
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
