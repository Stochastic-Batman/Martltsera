{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# `predict.py`\n",
    "\n",
    "This file implements inference, loading the trained model to correct words.\n",
    "\n",
    "The `SpellChecker` class loads the model, converts words to tensors (appending EOS), encodes the input, then decodes greedily (picking the highest-probability char each step) until EOS or a safety limit (50 chars).\n",
    "\n",
    "No teacher forcing here - it's pure autoregressive generation. The `correct_word` function wraps this for easy use.\n",
    "\n",
    "In testing, it shows corrections on examples like \"გამრჯობა\" (typo) $\\mapsto$ \"გამარჯობა\", demonstrating the model's ability to fix errors while preserving correct inputs."
   ],
   "id": "9b95f356a0480787"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T11:04:37.384432Z",
     "start_time": "2025-12-13T11:04:36.512824Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import logging\n",
    "import random\n",
    "import torch\n",
    "from src.Gamarjoba import Gamarjoba\n",
    "from src.get_data import ALL_GEORGIAN_CHARS\n",
    "\n",
    "\n",
    "random.seed(95)  # ⚡\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', datefmt='%d/%m/%Y %H:%M:%S')\n",
    "logger = logging.getLogger(\"MartltseraLogger (Inference)\")\n",
    "\n",
    "# constants and hyperparameters\n",
    "HIDDEN_SIZE = 512\n",
    "NUM_LAYERS = 1\n",
    "DROPOUT_P = 0.2\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "char_to_index = {char: i + 2 for i, char in enumerate(ALL_GEORGIAN_CHARS)}\n",
    "index_to_char = {i + 2: char for i, char in enumerate(ALL_GEORGIAN_CHARS)}\n",
    "VOCAB_SIZE = len(ALL_GEORGIAN_CHARS) + 2\n",
    "\n",
    "\n",
    "class SpellChecker:\n",
    "    def __init__(self, model_path: str = \"../models/Martltsera_5.pth\"):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = Gamarjoba(VOCAB_SIZE, HIDDEN_SIZE, num_layers=NUM_LAYERS, dropout_p=DROPOUT_P)\n",
    "        self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()  # important for dropout and inference behaviour\n",
    "\n",
    "\n",
    "    def tensor_from_word(self, word: str) -> torch.Tensor:\n",
    "        idxs = [char_to_index.get(char, EOS_token) for char in word if char in char_to_index]\n",
    "        idxs.append(EOS_token)\n",
    "        return torch.tensor(idxs, dtype=torch.long, device=self.device).view(-1, 1)\n",
    "\n",
    "\n",
    "    @staticmethod  # why not\n",
    "    def idx_to_char(idx: int) -> str:\n",
    "        return index_to_char.get(idx, \"\")\n",
    "\n",
    "\n",
    "    def fix(self, word: str) -> str:\n",
    "        if not word.strip():  # edge case\n",
    "            return word\n",
    "\n",
    "        with torch.no_grad():\n",
    "            input_tensor = self.tensor_from_word(word)\n",
    "\n",
    "            encoder_hidden, encoder_cell = self.model.encoder.init_hidden(self.device)\n",
    "            for i in range(input_tensor.size(0)):\n",
    "                _, encoder_hidden, encoder_cell = self.model.encoder(input_tensor[i], encoder_hidden, encoder_cell)\n",
    "\n",
    "            decoder_input = torch.tensor([[SOS_token]], device=self.device)\n",
    "            decoder_hidden = encoder_hidden\n",
    "            decoder_cell = encoder_cell\n",
    "\n",
    "            decoded_chars = []\n",
    "            while True:\n",
    "                prediction, decoder_hidden, decoder_cell = self.model.decoder(decoder_input, decoder_hidden, decoder_cell)\n",
    "                _, top_i = prediction.topk(1)\n",
    "                idx = top_i.squeeze().item()\n",
    "\n",
    "                if idx == EOS_token or len(decoded_chars) >= 50:  # safety limit\n",
    "                    break\n",
    "\n",
    "                decoded_chars.append(self.idx_to_char(idx))\n",
    "                decoder_input = top_i.squeeze().detach()  # feed prediction back as next input (no teacher forcing at inference)\n",
    "\n",
    "            return \"\".join(decoded_chars)\n",
    "\n",
    "\n",
    "def correct_word(word: str, model_path: str = \"../models/Martltsera_5.pth\") -> str:\n",
    "    model = SpellChecker(model_path=model_path)\n",
    "    return model.fix(word)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "logger.info(\"Inference examples:\")\n",
    "for i, w in enumerate([\"თბილისი\", \"საქონელი\", \"გამარჯობა\", \"გაგიმარჯოს\", \"ტელევიზია\", \"ბაყაყი\", \"მხედარი\", \"ნადირობა\", \"ნაპოლეონი\", \"ნარუტო\", \"შოთა\", \"აზარტი\", \"კომპიუტერი\", \"დაცვა\", \"ძლიერი\", \"ქართული\", \"ქართველი\", \"გამარჯვებულია\", \"მეცნიერი\", \"პროგრამისტი\"]):\n",
    "    logger.info(f\"{i + 1}) {w} -> {correct_word(w, model_path=\"../models/Martltsera_5.pth\")}\")"
   ],
   "id": "110659367a508e3e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13/12/2025 15:04:36 - MartltseraLogger (Inference) - INFO - Inference examples:\n",
      "13/12/2025 15:04:36 - MartltseraLogger (Inference) - INFO - 1) თბილისი -> თბილისი\n",
      "13/12/2025 15:04:36 - MartltseraLogger (Inference) - INFO - 2) საქონელი -> საქონელი\n",
      "13/12/2025 15:04:36 - MartltseraLogger (Inference) - INFO - 3) გამარჯობა -> გამარჯობა\n",
      "13/12/2025 15:04:36 - MartltseraLogger (Inference) - INFO - 4) გაგიმარჯოს -> გაგიმაროოს\n",
      "13/12/2025 15:04:36 - MartltseraLogger (Inference) - INFO - 5) ტელევიზია -> ტელევიიაია\n",
      "13/12/2025 15:04:36 - MartltseraLogger (Inference) - INFO - 6) ბაყაყი -> ბაყაყი\n",
      "13/12/2025 15:04:36 - MartltseraLogger (Inference) - INFO - 7) მხედარი -> მხედარი\n",
      "13/12/2025 15:04:36 - MartltseraLogger (Inference) - INFO - 8) ნადირობა -> ნადირობა\n",
      "13/12/2025 15:04:36 - MartltseraLogger (Inference) - INFO - 9) ნაპოლეონი -> ნაპოლეონი\n",
      "13/12/2025 15:04:36 - MartltseraLogger (Inference) - INFO - 10) ნარუტო -> ნარტულო\n",
      "13/12/2025 15:04:36 - MartltseraLogger (Inference) - INFO - 11) შოთა -> შოთა\n",
      "13/12/2025 15:04:37 - MartltseraLogger (Inference) - INFO - 12) აზარტი -> ააარტი\n",
      "13/12/2025 15:04:37 - MartltseraLogger (Inference) - INFO - 13) კომპიუტერი -> კომპიტერი\n",
      "13/12/2025 15:04:37 - MartltseraLogger (Inference) - INFO - 14) დაცვა -> დაცვა\n",
      "13/12/2025 15:04:37 - MartltseraLogger (Inference) - INFO - 15) ძლიერი -> ძლიერი\n",
      "13/12/2025 15:04:37 - MartltseraLogger (Inference) - INFO - 16) ქართული -> ქართული\n",
      "13/12/2025 15:04:37 - MartltseraLogger (Inference) - INFO - 17) ქართველი -> ქართველი\n",
      "13/12/2025 15:04:37 - MartltseraLogger (Inference) - INFO - 18) გამარჯვებულია -> გამარრვებულია\n",
      "13/12/2025 15:04:37 - MartltseraLogger (Inference) - INFO - 19) მეცნიერი -> მეცნიერი\n",
      "13/12/2025 15:04:37 - MartltseraLogger (Inference) - INFO - 20) პროგრამისტი -> პროგრამისტი\n"
     ]
    }
   ],
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
